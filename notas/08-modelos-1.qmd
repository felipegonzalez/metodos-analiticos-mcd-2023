# Modelos de espacio de estados

```{r}
#| code-fold: true
#| warning: false
library(tidyverse)
library(kableExtra)
library(DiagrammeR)
locale <- Sys.setlocale("LC_TIME", "es_ES.UTF-8")
library(lubridate)
library(fpp3)
ggplot2::theme_set(ggplot2::theme_light())
```

En esta parte veremos una introducción a los modelos bayesianos dinámicos de espacio
de estados. Estos modelos engloban otros enfoques (ARIMA, suavizamiento exponencial)
como casos particulares, y nos permite: 

- Modelar la estructura de una serie de tiempo de forma modular.
- Mayor flexibilidad en cuanto a las estructuras que podemos usar, y cómo usar
información acerca de la dinámica de la serie de tiempo (incluyendo aspectos de modelos ARIMA).
- Una forma estándar de tratar con valores faltantes, series irregulares, o varias observaciones contemporáneas.
- Una forma estándar de producir pronósticos puntuales, en intervalos, o por distribuciones.
- Una forma estándar de permitir que el modelo se adapte en el tiempo a cambios
estructurales (por ejemplo, cambios de patrones de estacionalidad, efectos diferentes
de covariables, etc.)

Aunque en muchos procesos tiene sentido considerar que la observación $y_t$ es
una función causal de observaciones pasadas, más generalmente podemos pensar
que existen *estados* que determinan el proceso generador de la serie. El estado
en el que se encuentra una proceso en un determinado momento puede tener varias
componentes, algunas estándar como nivel actual o periodo de estacionalidad,
pero también otras menos estándar, como niveles de otras variables.

## Modelo de nivel local

En primer lugar, consideraremos modelar la tendencia-ciclo de series con
el enfoque de espacio de estados. En primer lugar, tenemos una **ecuación
de observación**, que está dada por

$$y_t = \textrm{nivel}_t + \epsilon_t,$$
donde las $\epsilon_t \sim N(0,\sigma_\epsilon)$ independientes. Es decir, dado el nivel, las
observaciones $y_t$ son independientes. Adicionalmente tenemos una **ecuación de
transición de estados**, que en este caso es simplemente:

$$\textrm{nivel}_t = \textrm{nivel}_{t-1} + \eta_t$$
donde $\eta_t \sim N(0,\sigma_\eta)$ independientes.

Podemos hacer un diagrama como sigue para esta situación:

```{r}
#| code-fold: true
grViz('
digraph {
  graph [ranksep = 0.2]
  node [shape=circle]
    nt
    nt1
    nt2
    ntm1
    etm1
    et
    et1
    et2
  node [shape=plaintext]
    #    Y1 [label = <Y<SUB>1</SUB>>]
    dots [label = "..."]
    Ytm1 [label = <Y<SUB>t-1</SUB>>]
    Yt [label = <Y<SUB>t</SUB>>]
    Yt1 [label = <Y<SUB>t+1</SUB>>]
    Yt2 [label = <Y<SUB>t+2</SUB>>]
    etm1 [label = <e<SUB>t-1</SUB>>]
    et [label = <  e<SUB>t. </SUB>>]
    et1 [label = <e<SUB>t+1</SUB>>]
    et2 [label = <e<SUB>t+2</SUB>>]
        ntm1 [label = <n<SUB>t-1</SUB>>]
    nt [label = <  n<SUB>t  </SUB>>]
    nt1 [label = <n<SUB>t+1</SUB>>]
    nt2 [label = <n<SUB>t+2</SUB>>]
  edge [minlen = 3]
   #Y1 -> dots
   #dots -> Ytm1
   #Ytm1 -> Yt
   #Yt -> Yt1
   #Yt1 -> Yt2
   ntm1 -> Ytm1
   ntm1 -> nt
   nt1 -> Yt1
   nt -> Yt
   nt2 -> Yt2
   nt -> nt1
   nt1 -> nt2
   #e1 -> Y1
   etm1 -> Ytm1
   et -> Yt
   et1 -> Yt1
   et2 -> Yt2
{rank=same;  Ytm1; Yt; Yt1; Yt2}
{rank=max; ntm1; nt; nt1; nt2}
{rank=min; etm1; et; et1; et2}


}
', width = 300, height = 150)
```

Nótese que en esta gráfica, las $y_t$ no son independientes: están correlacionadas
a través de caminos abiertos que pasan por el estado, que en este caso
es el nivel de la serie.

::: callout-note
# Modelos de espacio de estados

Los modelos de espacio de estados para serie de tiempo consisten en
dos ecuaciones. Si $y_t$ son los datos observados y $\theta_t$ son
los parámetros que describen el estado, tenemos la ecuación de observación
$$y_{t} = f(\theta_{t}, \epsilon_t)$$
Es decir, cómo se relaciona el estado con las observaciones, y una
ecuación de evolución del estado:
$$\theta_{t} = g(\theta_{t-1}, \eta_t)$$
En los modelos dinámicos lineales (DLM), $f$ y $g$ son funciones lineales,
es decir
$$y_t = F_t\theta_t + \epsilon_t,$$
y

$$\theta_t = G_t\theta_{t-1} + R_t\eta_t,$$
donde suponemos, en general, que $\eta_t \sim N(0,\Sigma_t)$ y 
$\epsilon_t \sim N(0,S_t)$ son normales multivariadas independientes.
:::

**Nota**: más adelante veremos detalles de este planteamiento, en particular
para los DLMs, y por qué
aún cuando haya dependencias de más largo plazo en estado siempre es
posible escribir nuestros modelos de esta forma.

En el modelo de nivel local lo reescribimos por conveniencia como:

$$y_t = \alpha_t + \epsilon_t,$$
donde las $\epsilon_t \sim N(0,\sigma_\epsilon)$ independientes, y

$$\alpha_t = \alpha_{t-1} + \eta_t$$
donde $\eta_t \sim N(0,\sigma_\eta)$ independientes. En este caso, $F_t=1$ , $G_t=1$,
 y $R_t =1$ según la notación de arriba.
 

Para completar este modelo, es necesario:

1. Poner distribuciones previas al tamaño del ruido de la medición $p(\sigma_\epsilon)$ y a la evolución del nivel $p(\sigma_\eta)$.
2. Poner una distribución previa al estado inicial $p(\alpha_1)$


Usualmente consideramos que el nivel debe tener movimientos relativamente suaves,
de manera que $p(\sigma_\eta)$ debe estar adecuadamente concentrada cerca de 0.

## Análisis retrospectivo

Existen dos tipos de análisis que podemos hacer de una serie de
tiempo: análisis retrospectivo o análisis en línea. El primero es más
útil para entender el comportamiento de la serie del tipo dada toda
la información que tenemos, y el segundo es más útil cuando buscamos 
hacer pronósticos.

Comenzaremos estudiando los modelos de espacio de estados desde un punto
de vista retrospectivo.

::: callout-note
# Análisis retrospectivo

Una vez que tenemos todos los datos de la serie, ¿qué podemos
decir de su evolución en el tiempo?

Este análisis busca examinar las distribuciones $p(\theta_t|y_1,y_2,\ldots, y_T),$
es decir, examinar la posterior de los estados dado el conocimiento completo de la serie. 

:::



## Ejemplo: nivel del río Nilo

Empezamos con un ejemplo clásico y simple, que consiste en medidas anuales del
nivel del río Nilo. 

Usaremos *Stan* para ver los detalles de la implementación del modelo
de nivel local:

```{r}
#| message: false
library(cmdstanr)
source("../R/limpiar_draws.R")
mod_archivo <- "../src/series-de-tiempo/modelo-nivel-local.stan"
modelo_nivel <- cmdstan_model(mod_archivo)
read_lines(mod_archivo, skip = 20, n_max = 40) |> 
  cat(sep = "\n")
```


```{r}
#| message: false
sims <- modelo_nivel$sample(
  data = list(y = Nile, N = length(Nile),  s_obs = 200, q = 0.5, n_h = 4,
              ii_obs = 1:length(Nile), N_obs = length(Nile)),
  parallel_chains = 4, refresh = 1000, init = 0.1, step_size = 0.1,
  adapt_delta = 0.99)
```

Mostramos primero nuestra estimación de las desviaciones estándar
para el nivel y para la observación:

```{r}
sims$summary(c("sigma_obs", "sigma_nivel")) |> 
  select(variable, mean, q5, q95, rhat)
```

Y ahora resumimos y mostramos cómo se ve el nivel inferido bajo nuestro modelo.

```{r}
#| message: false
sims_nivel_tbl <- sims$draws(c("mu"), format = "df") |> 
  limpiar_draws(c("mu")) 
media_tbl <- sims_nivel_tbl |>  
  group_by(variable, t) |> 
  summarise(media = mean(valores), q5 = quantile(valores, 0.05),
            q95 = quantile(valores, 0.95))
```


```{r}
ggplot(sims_nivel_tbl |> filter(variable == "mu")) + 
  geom_line(aes(x = t, y = valores, group = .draw), 
            alpha = 0.01, size = 0.1, colour = "red") +
  geom_line(data = media_tbl, aes(x = t, y = media), colour = "red") +
  geom_point(data = tibble(y = Nile), aes(x = 1:length(Nile), y = Nile))
```

En esta gráfica:

- La línea roja es el valor esperado del nivel (que originalmente no es observado).
- La nube roja muestra varias simulaciones del nivel. Con esto entendemos cuánta incertidumbre tenemos acerca de la verdadera posición del nivel en cada momento.
- Nótese que la nube roja no tiene por qué cubrir los valores observados de la
serie, pues la ecuación de observación tiene incertidumbre adicional.
- Extendimos la estimación del estado para cuatro años adicionales. Nota cómo
conforme avanza el tiempo perdemos certidumbre acerca de dónde se encontrará el nivel.
- Nuestro modelo tiene 100 observaciones y 100 + 2 parámetros: 
las dos desviaciones estándar de modelos de observación y de
estado, y los 100 niveles no observados.

::: callout-tip
# Residuales y análisis retrospectivo

En el análisis retrospectivo, los residuales entre observaciones y
ajustados *no* se pueden interpretar como errores de predicción a un paso:
estos residuales incluyen información de toda la serie (valores pasados
y futuros del tiempo $t$ que estamos considerando).

:::


Podemos hacer chequeos predictivos posteriores simulando distintas
series a partir de los parámetros ajustados, y viendo si la estructura 
difiere (y cómo difiere) de los datos observados. En las siguientes gráficas,
sólo una de ellas tiene los datos reales, y el resto son simuladas del modelo:

```{r}
#| message: false
#| warning: false
obs_rep_tbl <- sims$draws(c("y_rep"), format = "df") |> 
  limpiar_draws(c("y_rep")) 
resumen_tbl <- obs_rep_tbl |> 
  filter(.draw <= 10) 
```
```{r}
resumen_tbl <- resumen_tbl |> 
  bind_rows(tibble(t = 1:length(Nile), valores = Nile, .draw = 11))
ggplot(resumen_tbl, aes(x = t, y = valores)) + geom_line() +
  facet_wrap(~ .draw)
```

Como es usual, cualquier resumen relevante puede usarse para hacer
chequeos predictivos posteriores, por ejemplo, las gráficas de acutocorrelación
de datos simulados (una caja tiene los verdaderos datos observados):

```{r}
resumen_tbl |> as_tsibble(index = t, key = .draw) |> 
  filter(.draw > 5, t <= 100) |> 
  ACF(valores) |> autoplot()
```

::: callout-note
# Diagnósticos usuales

- Es posible hacer chequeos posteriores predictivos como con cualquier modelo
bayesiano generativo.
- Veremos más adelante un diagnóstico importante que por ahora excluímos: la 
verificación de que las predicciones un paso hacia adelante cumplen ciertas
propiedades. Una de las más importantes es que no tengan autocorrelación.

:::

Finalmente, podemos ver cómo se ven **pronósticos** bajo este modelo. En este
caso, en la parte de cantidades generadas incluímos el código relevante. Nuestro
pronóstico podríamos graficarlo como sigue (con intervalos de 90%, por ejemplo):

```{r}
#| message: false
#| warning: false
pronosticos_tbl <- sims$draws(c("y_f"), format = "df") |> 
  limpiar_draws(c("y_f")) |> 
  group_by(variable, t) |> 
  summarise(media = mean(valores), q5 = quantile(valores, 0.05),
            q95 = quantile(valores, 0.95)) |> 
  mutate(t = t + length(Nile))
ggplot(sims_nivel_tbl |> filter(variable == "mu")) + 
  geom_line(aes(x = t, y = valores, group = .draw), 
            alpha = 0.01, size = 0.1, colour = "red") +
  geom_line(data = media_tbl, aes(x = t, y = media), colour = "red") +
  geom_point(data = tibble(y = Nile), aes(x = 1:length(Nile), y = Nile)) +
  geom_ribbon(data = pronosticos_tbl, aes(x = t, y = media, ymin = q5,
                                         ymax = q95), alpha = 0.1) +
  geom_point(data = pronosticos_tbl, aes(x = t, y = media))
```

### Ejemplo de desajuste {-}

Supongamos que en nuestro ejemplo no utilizáramos nivel dinámico,
sino constante.

```{r}
sims_desajuste <- modelo_nivel$sample(
  data = list(y = Nile, N = length(Nile),  s_obs = 200, q = 0.00001, 
              n_h = 4, ii_obs = 1:length(Nile), N_obs = length(Nile)),
  parallel_chains = 4, refresh = 1000, init = 0.1, step_size = 0.1,
  adapt_delta = 0.99)
```

Repetimos nuestros chequeos posteriores predictivos:

```{r}
obs_rep_tbl <- sims_desajuste$draws(c("y_rep"), format = "df") |> 
  limpiar_draws(c("y_rep")) 
resumen_tbl <- obs_rep_tbl |> 
  filter(.draw <= 10) #|> 
  #group_by(variable, t, indice, .draw) |> 
  #summarise(media = mean(valores), q5 = quantile(valores, 0.05),
  #          q95 = quantile(valores, 0.95))
```
```{r}
resumen_tbl <- resumen_tbl |> 
  bind_rows(tibble(t = 1:length(Nile), valores = Nile, .draw = 11))
ggplot(resumen_tbl, aes(x = t, y = valores)) + geom_line() +
  facet_wrap(~ .draw)
```

```{r}
resumen_tbl |> as_tsibble(index = t, key = .draw) |> 
  filter(.draw > 7, t <= 100) |> 
  ACF(valores) |> autoplot()
```

En todas estas gráficas podemos identificar claramente donde están los
datos. Nuestro modelo de nivel constante (no dinámico) no ajusta a estos datos simples.

## Valores faltantes

Existe una manera estándar de lidiar con valores faltantes en la serie
de tiempo cuando usamos modelos de espacio de estados:

- La ecuación de evolución del estado no cambia, la aplicamos aún cuando
no tengamos observaciones en esos tiempos.
- No incluimos términos en la posterior que involucran valores faltantes.

Por ejemplo, vamos a producir algunas mediciones faltantes en la serie del Río
Nilo:

```{r}
Nile_f <- Nile
Nile_f[50:58] <- NA
Nile_f[5:6] <- NA
ii_obs <- which(!is.na(Nile_f))
y_f <- ifelse(is.na(Nile_f), -1, Nile_f)
```

Y ajustamos el mismo modelo:

```{r}
#| message: false
sims <- modelo_nivel$sample(
  data = list(y = y_f, N = length(Nile_f),  s_obs = 200, q = 0.5, n_h = 4,
              N_obs = length(ii_obs),
              ii_obs = ii_obs),
  parallel_chains = 4, refresh = 1000, init = 0.1, step_size = 0.1,
  adapt_delta = 0.99)
```


```{r}
#| message: false
#| warning: false
sims_nivel_tbl <- sims$draws(c("mu"), format = "df") |> 
  limpiar_draws(c("mu")) 
media_tbl <- sims_nivel_tbl |>  
  group_by(variable, t) |> 
  summarise(media = mean(valores), q5 = quantile(valores, 0.05),
            q95 = quantile(valores, 0.95))
```


```{r}
ggplot(sims_nivel_tbl |> filter(variable == "mu")) + 
  geom_line(aes(x = t, y = valores, group = .draw), 
            alpha = 0.02, size = 0.1, colour = "red") +
  geom_line(data = media_tbl, aes(x = t, y = media), colour = "red") +
  geom_point(data = tibble(y = Nile_f), aes(x = 1:length(Nile), y = y))
```

- En este caso, vemos que el la posterior del nivel en los valores
desconocidos se amplía, lo cual refleja el hecho de que no tenemos
observaciones que informen de su localización.


## Intervenciones

Cuando consideramos análisis secuencial (veremos más adelante) y 
detectamos desajustes o cambios
grandes en la serie, es posible agregar incertidumbre en esos momentos,
lo cual permite al modelo hacer evaluar reajustes de los parámetros que
son más consistentes con datos y modelo. En este ejemplo, en el tiempo
29 es posible diagnosticar un cambio de nivel que es incosistente con lo
observado anteriormente. Podemos hacer una intervención en ese tiempo
ampliando la varianza en la evolución del nivel:

- En este caso, el modelo de observación no es modificado.
- Ampliamos la varianza en momentos donde esperamos cambios de estado grandes (en este caso el nivel).

```{r}
mod_archivo_int <- "../src/series-de-tiempo/modelo-nivel-local-int.stan"
modelo_nivel <- cmdstan_model(mod_archivo_int)
read_lines(mod_archivo_int, skip = 20, n_max = 25) |> 
  cat(sep = "\n")
```


```{r}
#| message: false
sims <- modelo_nivel$sample(
  data = list(y = Nile, N = length(Nile),  s_obs = 200, q = 0.5, n_h = 4,
              ii_obs = 1:length(Nile), N_obs = length(Nile),
              t_intervencion = 29),
  parallel_chains = 4, refresh = 1000, init = 0.1, step_size = 0.1,
  adapt_delta = 0.99)
```

Mostramos primero nuestra estimación de las desviaciones estándar
para el nivel y para la observación:

```{r}
#| message: false
#| warning: false
sims$summary(c("sigma_obs", "sigma_nivel")) |> 
  select(variable, mean, q5, q95)
```

Y ahora resumimos y mostramos cómo se ve el nivel inferido bajo nuestro modelo.

```{r}
#| message: false
#| warning: false
sims_nivel_tbl <- sims$draws(c("mu"), format = "df") |> 
  limpiar_draws(c("mu")) 
media_tbl <- sims_nivel_tbl |>  
  group_by(variable, t) |> 
  summarise(media = mean(valores), q5 = quantile(valores, 0.05),
            q95 = quantile(valores, 0.95))
```


```{r}
#| fig-width: 7
#| fig-height: 3
ggplot(sims_nivel_tbl |> filter(variable == "mu")) + 
  geom_line(aes(x = t, y = valores, group = .draw), 
            alpha = 0.01, size = 0.1, colour = "red") +
  geom_line(data = media_tbl, aes(x = t, y = media), colour = "red") +
  geom_point(data = tibble(y = Nile), aes(x = 1:length(Nile), y = Nile))
```

- En este modelo, la evolución general del nivel es más suave, pues
ahora estamos absorbiendo el cambio de nivel con una intervención en 
la varianza al tiempo 29.
- Nótese que la única modificación al modelo es ampliar la incertidumbre 
acerca del estado al tiempo 29. 
- Compara la estimación de los parámetros $\sigma_\eta, \sigma_epsilon$ en
los dos modelos: como es de esperarse, en este segundo modelo con un punto
de cambio la evolución del nivel es más suave.


## Modelo de nivel local con tendencia

El siguiente modelo que consideramos aplica a series que tienen tendencia
y ciclos más definidos. En este caso, la ecuación de observación es la misma:

$$y_t = \mu_t + \epsilon_t,$$
pero las ecuaciones de evolución de estado son ahora

$$\mu_t = \mu_{t-1} + \nu_{t-1} + \eta_{1,t}$$
$$\nu_t = \nu_{t-1} + \eta_{2, t}$$
Nótese que en este caso buscamos aprender tendencias además de nivel local. Las
tendencias son localmente lineales, pero pueden adaptarse en el tiempo
con la estructura de la serie.

En este caso veremos un ejemplo de @commandeur2007introduction, donde
se examinan muertes por accidentes automovilísticos en Finlandia.

```{r}
#| fig-width: 7
#| fig-height: 3
#| message: false
finnish_tbl <- read_table("../datos/NorwayFinland.txt") |> 
  as_tsibble(index = year)
head(finnish_tbl)
autoplot(finnish_tbl, Finnish_fatalities)
```
```{r}
modelo_tend <- "../src/series-de-tiempo/modelo-nivel-local-tend.stan"
mod_1 <- cmdstan_model(modelo_tend)
read_lines(modelo_tend, skip = 13, n_max = 43) |> 
  cat(sep = "\n")
```


```{r}
N <- nrow(finnish_tbl)
sims <- mod_1$sample(
  data = list(y = finnish_tbl$Finnish_fatalities,  
              N = N,  s_obs = 200, 
              q_nivel = 0.05,
              q_tend = 0.05,
              n_h = 3,
              ii_obs = 1:N, N_obs = N),
  iter_sampling = 3000, iter_warmup = 3000,
  parallel_chains = 4, refresh = 1000,
  adapt_delta = 0.9995)
```

```{r}
sims$summary(c("sigma_obs", "sigma_nivel", "sigma_tend")) |> 
  select(variable, mean, q5, q95, rhat) |> 
  mutate(across(where(is_double), ~ round(.x, 3)))
```



```{r}
#| message: false
#| warning: false
tiempo_tbl <- select(finnish_tbl, year) |> 
  mutate(t = row_number()) |> 
  bind_rows(tibble(t = 35:37, year = 2004:2006))
sims_nivel_tbl <- sims$draws(c("mu"), format = "df") |> 
  limpiar_draws(c("mu")) |> 
  left_join(tiempo_tbl)
media_tbl <- sims_nivel_tbl |>  
  group_by(variable, t, year) |> 
  summarise(media = mean(valores), q5 = quantile(valores, 0.05),
            q95 = quantile(valores, 0.95))
```

Notamos ahora que nuestro pronóstico tiene una tendencia creciente,
pues la incluimos en el modelo:

```{r}
#| fig-width: 7
#| fig-height: 3
ggplot(sims_nivel_tbl |> filter(variable == "mu")) + 
  geom_line(aes(x = year, y = valores, group = .draw), 
            alpha = 0.01, size = 0.1, colour = "red") +
  geom_line(data = media_tbl, aes(x = year, y = media), colour = "white") +
  geom_point(data = finnish_tbl, aes(x = year, y = Finnish_fatalities))
```
Podemos ver los cambios de tendencia en el modelo como componente separada,
donde vemos que en general la tendencia es a la baja, aunque entre 1980
y 1990 tuvo periodos de incremento:

```{r}
#| message: false
#| warning: false
sims_nivel_tbl <- sims$draws(c("nu"), format = "df") |> 
  limpiar_draws(c("nu")) |> 
  left_join(tiempo_tbl)
ggplot(sims_nivel_tbl |> filter(variable == "nu", year < 2004)) + 
  geom_line(aes(x = year, y = valores, group = .draw), 
            alpha = 0.01, size = 0.05, colour = "red") +
  geom_hline(yintercept = 0) + ylab("Tendencia") 
```

::: callout-note
# Nivel local con tendencia

Para escribir el modelo de nivel local con tendencia, el estado
se puede definir como el vector

$$
\theta_t  = 
\begin{pmatrix}
\mu_t \\
\nu_t
\end{pmatrix}
$$
y podemos
utilizar nuestra notación de arriba con:

$$
F_t = F = 
\begin{pmatrix}
1 \\
0
\end{pmatrix},\, G_t = G = 
\begin{pmatrix}
1 & 1\\
0 & 1
\end{pmatrix}
$$
:::

La ecuación de observación es entonces

$$
y_t = \begin{pmatrix}
1 & 0
\end{pmatrix}
\begin{pmatrix}
\mu_t \\
\nu_t
\end{pmatrix} + \epsilon_t
$$
y la ecuación de evolución del estado es

$$
\begin{pmatrix}
\mu_{t+1}  \\
\nu_{t+1}
\end{pmatrix} = 
\begin{pmatrix}
1 & 1\\
0 & 1
\end{pmatrix}
\begin{pmatrix}
\mu_{t}  \\
\nu_{t}
\end{pmatrix}
+
\begin{pmatrix}
\eta_{1,t} \\
\eta_{2,t}
\end{pmatrix} 
$$



## Modelos con estacionalidad fija

Antes de mostrar estacionalidad dinámica, mostramos cómo construiríamos
el espacio de estados para un modelo con nivel local, sin tendencia y con
estacionalidad *fija*. El modelo de observación
será ahora:

$$y_t = \mu_t + \gamma_{1,t} + \epsilon_t$$
donde $\gamma_{1,t}$ es el coeficiente de estacionalidad. 

Para simplificar por el momento, 
consideramos datos en trimestres. Como la estacionalidad es fija,
 hay cuatro parámetros $\beta_1,\beta_2,\beta_3,\beta_4$, que deben
 ser parte del espacio de estados, tales
que (suponiendo que la serie comienza en el primer trimestre):

Para $t=1$, los estados los pondremos como

$$
\gamma_{1,1} = \beta_1,\gamma_{2,1} =\beta_2,\gamma_{3,1} = \beta_3,\gamma_{4,1} = \beta_4
$$
así que para $t=2$ podemos poner:

$$
\gamma_{1,2} = \beta_2,\gamma_{2,2} =\beta_3,\gamma_{3,2} = \beta_4,\gamma_{4,2} = \beta_1
$$
$$
\gamma_{1,3} = \beta_3,\gamma_{2,3} =\beta_4,\gamma_{3,3} = \beta_1,\gamma_{4,3} = \beta_2
$$
y así sucesivamente, los coeficientes del estado para la estacionalidad
$\gamma_{1,t},\gamma_{2,t},\gamma_{3,t},\gamma_{4,t}$ son una rotación
de $\beta_1,\beta_2,\beta_3,\beta_4$.


Podemos escribir entonces el estado (suponiendo nivel local sin tendencia) como sigue:

$$\theta_1^* = (\mu_1, \beta_1, \beta_2, \beta_3, \beta_4)^t = (\mu_1, \gamma_{1,1}, \gamma_{2,1}, \gamma_{3,1}, \gamma_{4,1})^t,$$
$$\theta_2^* = (\mu_2, \beta_2, \beta_3, \beta_4, \beta_1)^t = (\mu_2, \gamma_{1,2}, \gamma_{2,2}, \gamma_{3,2}, \gamma_{4,2})^t,$$
$$\theta_3^* = (\mu_3, \beta_3, \beta_4, \beta_1, \beta_2)^t = (\mu_3, \gamma_{1,3}, \gamma_{2,3}, \gamma_{3,3}, \gamma_{4,3})^t,$$
Observando estas ecuaciones,  la matriz de evolución del
sistema es una matriz que rota las componentes correspondientes a la estacionalidad:

$$
\theta_{t+1}^* = 
G_t \theta_t^* + 
\begin{pmatrix}
\eta_t \\
0 \\
0 \\
0 \\
0 \\
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
0 & 1 & 0 & 0 & 0\\
\end{pmatrix}
\theta_t^* +
\begin{pmatrix}
\eta_t \\
0 \\
0 \\
0 \\
0 \\
\end{pmatrix}
$$
Puedes verificar qué pasa con los vectores $\theta_2^*,\theta_3^*$ mostrados arriba. La matriz $F_t$ tiene que sacar el primer valor de estacionalidad $\gamma_{1,t}$ para concordar con el modelo de observaciones:

$$
y_t = \mu_t + \gamma_{1,t} + \epsilon_t = F_t\theta_t^* + \epsilon_t=
\begin{pmatrix}
1 & 1 & 0 & 0 & 0
\end{pmatrix}
\theta_t^* + \epsilon_t
$$
El único problema con este modelo de estacionalidad fija es 
que está sobreparametrizado. Por razones de ajuste y de interpretación,
es más conveniente que se cumpla alguna restricción, por ejemplo:

$$\beta_1 + \beta_2 + \beta_3 + \beta_4 =0,$$

de modo que para cualquier $t$, tenemos que:

$$\gamma_{1,t} + \gamma_{2,t} + \gamma_{3,t} + \gamma_{4,t}=0.$$
Podemos reducir entonces nuestro espacio de estados en una dimensión,
pues siempre uno de los coeficientes en cada periodo es el negativo de la suma
del resto. Por ejemplo, si nuestro nuevo espacio de estado lo definimos como

$$
\theta_1 = (\mu_1, \gamma_{1,1}, \gamma_{4,1}, \gamma_{3,1})^t
$$
entonces (haz el cálculo sustiyendo las $\beta$ fijas si este argumento te confunde):

$$
\theta_{2} = (\mu_{2}, \gamma_{1,2}, \gamma_{4,2}, \gamma_{3,2})^t = (\mu_2, 
-(\gamma_{1,1} + \gamma_{4,1} + \gamma_{3,1}),
\gamma_{1,1}, \gamma_{4,1})^t
$$
Y en términos matriciales

$$
\theta_{t+1} = G_t\theta_t +
\begin{pmatrix}
\eta_t \\
0 \\
0 \\
0 \\
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & -1 & -1 & -1 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{pmatrix}
\theta_t
+
\begin{pmatrix}
\eta_t \\
0 \\
0 \\
0 \\
\end{pmatrix},
$$
y así sucesivamente para $t=1,2,\ldots$.

## Modelos con estacionalidad dinámica

Para hacer la estacionalidad dinámica, podemos agregar simplemente

$$
\theta_{t+1} = G_t\theta_t +
\begin{pmatrix}
\eta_t \\
\omega_t \\
0 \\
0 \\
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & -1 & -1 & -1 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{pmatrix}
\theta_t
+
\begin{pmatrix}
\eta_t \\
\omega_t \\
0 \\
0 \\
\end{pmatrix},
$$

es decir, le agregamos $\gamma_{1,t+1} = - (\gamma_{1,t} +\gamma_{2,t}+\gamma_{3,t})$ una perturbación $\omega_t \sim N(0, \sigma_\omega)$. Simplificando la notación,
si la ecuación de observación es

$$y_t = \mu_t + \gamma_t + \epsilon_t.$$
la ecuación de transición para el término $\gamma_t$ de estacionalidad es

$$\gamma_t = - (\gamma_{t-3} + \gamma_{t-2} + \gamma_{t-1}) + \omega_t$$


Abajo podemos ver una implementación en Stan:

```{r}
modelo_est <- "../src/series-de-tiempo/modelo-nivel-local-tend-est.stan"
mod_est_1 <- cmdstan_model(modelo_est)
read_lines(modelo_est, skip = 28, n_max = 42) |> 
  cat(sep = "\n")
```

### Ejemplo: cinturones de seguridad

En este ejemplo también clásico examinamos la serie de tiempo
muertes de conductores en UK (que viajan en asientos traseros del automóvil):

```{r}
uk_drivers <- Seatbelts |> as_tsibble()
head(uk_drivers)
uk_rear <- uk_drivers |> filter(key == "rear")
```
```{r}
#| message: false
#|
sims <- mod_est_1$sample(
  data = list(y = uk_rear$value,  
              N = nrow(uk_rear),  s_obs = 50, 
              q_nivel = 0.5,
              q_tend = 1e-6, # sin tendencia
              n_h = 6,
              ii_obs = 1:nrow(uk_rear), N_obs = nrow(uk_rear),
              periodo = 12),
  iter_sampling = 1000, iter_warmup = 1000,
  parallel_chains = 4, refresh = 1000,
  adapt_delta = 0.99)
```



```{r}
sims$summary(c("sigma_obs", "sigma_nivel", "sigma_tend", "sigma_est")) |> 
  select(variable, mean, q5, q95, rhat)  |> 
  mutate(across(where(is_double), ~ round(.x, 3)))
```


```{r}
#| message: false
#| warning: false
sims_nivel_tbl <- sims$draws(c("alpha"), format = "df") |> 
  limpiar_draws(c("alpha"))
sims_ajuste_tbl <- sims$draws(c("mu"), format = "df") |> 
  limpiar_draws(c("mu"))
media_tbl <- sims_ajuste_tbl |>  
  group_by(variable, t) |> 
  summarise(media = mean(valores), q5 = quantile(valores, 0.05),
            q95 = quantile(valores, 0.95))
```

Veamos nuestro ajuste y pronóstico:

```{r}
ggplot(sims_ajuste_tbl |> filter(variable == "mu")) + 
  geom_line(aes(x = t, y = valores, group = .draw), 
            alpha = 0.01, size = 0.1, colour = "red") +
  geom_line(data = media_tbl, aes(x = t, y = media), colour = "orange") +
  geom_point(data = Seatbelts |> as_tibble() |> mutate(t = row_number()), 
             aes(x = t, y = rear))
```
Podemos también graficar la componente de estacionalidad, que vemos
que es considerablemente estable:

```{r}
#| message: false
#| warning: false
sims_est_tbl <- sims$draws(c("gamma"), format = "df") |> 
  limpiar_draws(c("gamma"))
media_tbl <- sims_est_tbl |>  
  group_by(variable, t) |> 
  summarise(media = mean(valores), q5 = quantile(valores, 0.05),
            q95 = quantile(valores, 0.95))
ggplot(sims_est_tbl |> filter(variable == "gamma")) + 
  geom_line(aes(x = t, y = valores, group = .draw), 
            alpha = 0.01, size = 0.05, colour = "red") + 
  geom_line(data = media_tbl, aes(x = t, y = media), colour = "black") 
```



## Construcción modular de modelos


Agregando este nuevo módulo de estacionalidad a nuestro modelo de nivel 
local con tendencia, obtenemos
que la ecuación de observación es ahora (para periodo $s=4$):

$$
y_t = \begin{pmatrix}
1 & 0 & 1 & 0 & 0 & 0
\end{pmatrix}
\theta_t + \epsilon_t
$$
donde 

$$
\theta_{t+1} = 
\begin{pmatrix}
\mu_{t+1} \\
\nu_{t+1} \\ 
\gamma_{t+1} \\
\gamma_{t}\\
\gamma_{t-1}\\
\end{pmatrix}
= G \theta_t +R\eta_t = 
G
\begin{pmatrix}
\mu_t \\
\nu_t \\ 
\gamma_{t} \\
\gamma_{t-1}\\
\gamma_{t-2}\\
\end{pmatrix} +
R
\begin{pmatrix}
\eta_{1,t} \\
\eta_{2,t} \\
\omega_{t} \\
\end{pmatrix}
$$
donde

$$
G=
\begin{pmatrix}
1 & 1 & & & \\
0 & 1 &\\ 
 & & &  -1 & -1 & -1\\
 & & &  1 & 0 & 0\\
 & & & 0 & 1 & 0 \\
\end{pmatrix}
$$
y 

$$
R=
\begin{pmatrix}
1 & 0 & & & \\
0 & 1 &\\ 
 & & & 1 \\ 
 & & & 0\\ & & & 0 \\ & & & 0\\
\end{pmatrix}
$$

Como vemos, las matrices $G_t$ y $F_t$ están organizadas en bloques,
una para cada componente del modelo. 

De esta manera, podemos agregar distintas componentes a nuestros modelos
de manera modular. La matriz $G$ está organizada en bloques a lo a largo
de su diagonal, un bloque para cada componente del modelo. Igualmente,
la matriz $F$ está separada por bloques que corresponden a las componentes.

### Ejemplo {-}

En las aplicaciones originales de seguimiento de objetos basado en sensores
ruidosos, el estado que nos interesa es la posición y velocidad de un objeto:

$$\theta_t = (x_{i,t}, y_{i, t}, \dot x_{i,t}, \dot y_{i,t})$$
Con matriz de transición:

$$
G = 
\begin{pmatrix}
1 & 0 & \delta_t & 0 \\
0 & 1 &   0 & \delta_t\\
0 & 0 &   1 & 0\\
0 & 0 &   0 & 1\\
\end{pmatrix}
$$
donde $\delta_t$ es el tiempo transcurrido entre mediciones. Cada una
de estos tiene una perturbación aleatoria. Para la
ecuación de observación, si las posiciones
medidas por el sensor son pares de coordenadas $y_{1,t},y_{2,t}$, y la matriz
$F$ es entonces

$$
F = 
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 &   0 & 0\\
\end{pmatrix}
$$
y en este ejemplo podemos usar errores normales con varianza conocida (precisión
del sensor).

Si recordamos nuestros programas de Stan, nota que nada impide que podamos
utilizar lecturas de varios sensores. Simplemente es necesario ponerlos
como datos que informan los mismos parámetros. La estructura del error puede
ser independiente o correlacionada, dependiendo de la naturaleza de los 
distintos sensores.



## Componentes de regresión dinámica

Finalmente, veremos cómo agregar variables explicativas a modelos, lo
cual puede ser especialmente útil para análisis retrospectivo, o cuando
tales variables están disponibles antes que las que queremos 
pronosticar (como en *nowcasting*).

En nuestro primer ejemplo, pretendemos medir el efecto de la ley
de cinturones de seguridad en UK. Utilizaremos la variable de muertes
y heridos de gravedad de pasajeros en la parte frontal del automóvil:

```{r}
autoplot(uk_drivers |> filter(key %in% c("front", "rear")))
uk_front <- filter(uk_drivers, key == "front")
law <- filter(uk_drivers, key == "law")
```



```{r}
modelo_est <- "../src/series-de-tiempo/modelo-seatbelts-reg-estatica.stan"
mod_est_1 <- cmdstan_model(modelo_est)
read_lines(modelo_est, skip = 28, n_max = 42) |> 
  cat(sep = "\n")
```


```{r}
#| message: false
sims <- mod_est_1$sample(
  data = list(y = log(uk_front$value),  
              N = nrow(uk_front),  s_obs = 1, 
              q_nivel = 0.5,
              q_tend = 0.1,
              q_reg = 0.5,
              n_h = 0,
              ii_obs = 1:nrow(uk_front), N_obs = nrow(uk_front),
              periodo = 12,
              x = law$value),
  iter_sampling = 2000, iter_warmup = 2000,
  parallel_chains = 4, refresh = 1000, init = 0.01,
  step_size = 0.01,
  adapt_delta = 0.99)
```


```{r}
sims$summary(
  c("sigma_obs", "sigma_nivel", "sigma_tend", "sigma_est", 
    "beta")) |> 
  select(variable, mean, q5, q95, rhat) |> 
  mutate(across(where(is.double), ~ round(.x, 6)))
```

El coeficiente de la variable dummy que indica
la presencia de la ley de usar cinturón de seguridad redujo en alrededor de 35%
el número de muertes o heridos graves en accidentes de pasajeros viajando
en los asientos delanteros,
controlando por cambios posibles de nivel de la serie y estacionalidad.

```{r}
sims_est_tbl <- sims$draws(c("mu_sin_est", "alpha"), format = "df") |> 
  limpiar_draws(c("mu_sin_est", "alpha"))
media_tbl <- sims_est_tbl |>  
  group_by(variable, t) |> 
  summarise(media = mean(valores), q5 = quantile(valores, 0.05),
            q95 = quantile(valores, 0.95))

ggplot(sims_est_tbl |> filter(variable == "mu_sin_est")) + 
  geom_line(aes(x = t, y = valores, group = .draw), 
            alpha = 0.01, size = 0.05, colour = "red") +
  geom_line(data = media_tbl, aes(x = t, y = media, colour = variable)) +
  geom_vline(xintercept = which(law$value==1)[1], colour = "gray") +
   geom_point(data = Seatbelts |> as_tibble() |> mutate(t = row_number()), 
             aes(x = t, y = log(front)))
```

En este ejemplo usamos el modelo de observaciones dado por

$$y_t = \mu_t + \gamma_t + \beta x_t + \epsilon_t$$
dond $x_t$ es la variable dummy que indica la aplicación de la ley de
cinturones de seguridad. En general, podemos usar más covariables
apropiadas y coeficientes dinámicos:

$$y_t = \mu_t + \gamma_t + \beta_t x_t + \epsilon_t$$
En este caso, podemos definir el estado como:

$$\theta_t = (\mu_t, \nu_t, \gamma_{1,t},\ldots,\gamma_{s,t}, \beta_{1,t},\beta_{2,t},
\ldots,\beta_{p,t})^t,$$

La ecuación de observación es:

$$y_t = F_t\theta_t + \epsilon_t,$$

donde 
$$
F_t = 
(1, 0, 1, 0,\ldots, 0, x_{1,t}, x_{2,t},\ldots , x_{p,t}).
$$

La ecuación de transición de estados (suponiendo datos trimestrales
($s=4$) y dos covariables $x_t$ ($p=2$) está dada por 

$$
G_t =
\begin{pmatrix}
1 & 1 & & & & &\\
0 & 1 & & &\\ 
 & & &  -1 & -1 & -1 & &\\
 & & &  1 & 0 & 0 & &\\
 & & & 0 & 1 & 0 & &\\
  & & &  &  &  & 1& 0\\
 & & &  &  &  & 0 & 1\\

\end{pmatrix}
$$

Para términos $\beta$ estáticos de regresión, podemos tomar

$$
\theta_{t} = G_t \theta_{t-1} + 
\begin{pmatrix}
\eta_{1,t} \\
\eta_{2,t} \\
\omega_t \\
0 \\
\vdots \\
0

\end{pmatrix}
$$

y para regresión dinámica:

$$
\theta_{t} = G_t \theta_{t-1} + 
\begin{pmatrix}
\eta_{1,t} \\
\eta_{2,t} \\
\omega_t \\
0 \\
\vdots \\
0\\
e_{1,t} \\
e_{2,t} \\
\end{pmatrix}
$$

con $e_{i,t} \sim N(0, \sigma_{i,\beta})$. Es decir, ponemos una transición
de caminata aleatoria para cada coerficiente. Generalmente:

- Conviene que las variables $x_i$ estén centradas y escaladas (estandarizadas) para que tengan variación similar
- Al centrarlas, evitamos problemas en el ajuste por correlaciones con el nivel
y tendencia.
- Al escalarlas, es más simple poner iniciales apropiadas para $\sigma_{i,\beta}$.



### Ejemplo: cinturones de seguridad

Regresamos al ejemplo de cinturones de seguridad. Usaremos el paquete 
*bsts* para especificar nuestro modelo. En este ejemplo usaremos
adicionalmente la variable de precio de gasolina, pues esta variable
tiene efecto sobre la forma de manejar y las decisiones de viajes.

```{r}
#| message: false
library(bsts)
y <- log(uk_front$value) |> as.ts()
precio_gas <- log(uk_drivers |> filter(key == "PetrolPrice") |> 
  pull(value))
precio_gas_c <- (precio_gas - mean(precio_gas))
```

```{r}
ggplot(uk_drivers |> filter(key == "PetrolPrice"), 
       aes(x = index, y = value)) +
  geom_point() +
  geom_vline(xintercept = my("Feb, 1983"), colour = "red")
```



```{r}
ley <- uk_drivers |> filter(key=="law") |> pull(value)
esp_estado <- 
  AddLocalLinearTrend(list(), y) |> 
  AddSeasonal(y, nseasons = 12) |>
  AddDynamicRegression(y ~ precio_gas_c + ley) 
```

```{r}
ajuste <- bsts(y, state.specification = esp_estado,
               niter = 10000, seed = 2323)
```

```{r}
plot(ajuste, "state", burn = 2000)
```

Y podemos ver los coeficientes de los términos de regresión:

```{r}
plot(ajuste, "dynamic", burn = 1000)
```
El coeficiente de precio de la gasolina es negativo, y se reduce
en cierta medida en el tiempo.

Podemos graficar simplemente las componentes de nuestro modelo:

```{r}
plot(ajuste, "components")
PlotBstsComponents(ajuste, components = 2:3, same = FALSE)
```
O también podemos graficar manualmente, por ejemplo:

```{r}
dims <- ajuste$state.contributions |> dim()
tiempo <- dims[3]
contribuciones_tbl <- map(1:tiempo, ~ ajuste$state.contributions[,,.x] |> 
  as_tibble() |> mutate(t = .x)) |> bind_rows() |> 
  pivot_longer(trend:dynamic, values_to = "value", names_to = "comp") |> 
  group_by(t, comp) |> 
  summarise(media = mean(value), q5 = quantile(value, 0.05),
            q95 = quantile(value, 0.95), .groups = "drop")
```


En la siguiente gráfica mostramos las componentes *en escala diferente*:

```{r}
ggplot(contribuciones_tbl, 
  aes(x = t, y = media, ymin = q5, ymax = q95)) +
  geom_ribbon(alpha = 0.1) + 
  geom_line(alpha = 1) + facet_wrap(~ comp, scales = "free_y", ncol = 1)
```
Nótese que cerca de la intervención se amplía el intervalo del nivel: esto
es porque existe la posibilidad de que que la tendencia explique (o contrarrestre)
parte del efecto de la introducción de la ley de cinturones.

## Análisis secuencial

Ahora veremos cómo hacemos análisis secuencial, en contraste al análisis
retrospectivo que examinamos en la parte anterior. Este análisis, además
de ser útil en la práctica, es importante pues produce diagnósticos 
útiles, relativamente simples, para determinar qué tan bueno es el ajuste
del modelo a las series de tiempo de interés.

Veremos el argumento aplicado al modelo del nivel local. Puedes
ver el argumento general en la sección 4.2 de @durbinkoopman.

$$y_t =  \theta_t + \epsilon_t,$$
$$\theta_t = \theta_{t-1} + \eta_t,$$
con 

- $\epsilon_t$ normal con media cero y varianza $\sigma_\epsilon^2$, y 
- $\eta_t$ normal con media cero y varianza $\sigma_\eta^2$.
- En esta parte, supondremos $\sigma_\epsilon^2$ y $\sigma_\eta^2$ **conocidas** (aunque en los
ejemplos anteriores no lo hicimos). 

Denotaremos todos los valores de la serie hasta el momento
$t$, $y_1,y_2,\ldots, y_t$ mediante $D_t$. Nótese que todo el 
análisis retrospectivo que hemos hecho hasta el momento es de la posterior
de los estados dados todos los datos:
$$p(\theta_t|D_T)$$
Para hacer el análisis en línea, supongamos que estamos en el momento $t$ y 
observamos $y_t$. Consideramos las siguientes distribuciones:

- Predicción a un paso del estado del sistema: $p(\theta_t| D_{t-1})$
- Predicción del valor de la serie a un paso: $p(y_t | D_{t-1})$
- Estado actualizado cuando observamos $y_t$: $p(\theta_t | D_t)$

Supondremos que tenemos $p(\theta_{t-1}|D_{t-1})$, y que tiene
distribución normal con media $m_{t-1}$ y varianza $C_{t-1}$:

$$\theta_{t-1}|D_{t-1} \sim N(m_{t-1}, C_{t-1}).$$

Ahora veremos cómo encontramos
las distribuciones mostradas arriba, lo cuál nos dará una manera recursiva
de hacer estos cálculos.

### Evolución del estado

Encontrar $p(\theta_t|D_{t-1})$ es relativamente fácil. Como

$$\theta_t = \theta_{t-1}+ \eta_t$$,

Entonces (por propiedades de esperanza y varianza):

$$\theta_t|D_{t-1} \sim N(m_{t-1}, C_{t-1} + \sigma_\eta^2)$$
Denotamos $a_t$ y $P_t$ como la media y varianza de $\theta_t|D_{t-1}$,
de modo que tenemos las ecuaciones
$$a_t = m_{t-1}, P_t = C_{t-1} + \sigma_\eta^2$$ 

### Actualización

Ahora consideramos actualizar nuestra información del estado incluyendo
$y_t$ una vez que lo observamos, es decir, queremos calcular 
$p(\theta_t|D_t)$. Por la regla de Bayes,

$$p(\theta_t|D_t)=p(\theta_t|D_{t-1},y_t) \propto p(Y_t=y_t|D_{t-1}, \theta_t)p(\theta_t|D_{t-1})$$
Esto es el producto de dos distribuciones normales, así que el producto es normal,
y basta con encontrar su media $m_t$ y varianza $C_t$.
Podemos usar esta fórmula y completar cuadrados. Daremos otro argumento abajo:

Consideramos primero $E[\theta_t|D_t] = E_{D_{t-1}}[\theta_t| y_t]$. La ecuación
de espacio de estados indica que dado $D_{t-1}$, $y_t$ y $\theta_t$ tienen
una distribución conjunta bivariada (la condicional de $\theta_t$ dado $D_{t-1}$ es
normal como vimos arriba, y la condicional de $y_t$ dado $D_{t-1}$ y $\theta_t$
es normal por la ecuación de observación).

::: callout-tip

Recordamos entonces que si $Y$ y $Z$ tienen
una distribución normal bivariada, entonces la condicional de $Y$ 
dada $Z$ tiene una distribución normal
con media

$$E[Y|Z] = E[Y] + \rho\frac{\sigma_Y}{\sigma_Z}(Z -  E[Z])$$
y varianza
$$V(Y|Z=z) = ({1-\rho^2}){\sigma_Y^2}$$
:::

Aplicando a nuestro caso, tenemos

$$E_{D_{t-1}}[\theta_t| y_t] = E_{D_{t-1}}[\theta_t] + \rho\sqrt{\frac{\textrm{Var}_{D_{t-1}}(\theta_t)}{\textrm{Var}_{D_{t-1}}(y_t)}}\left
(y_t - E_{D_{t-1}}[y_t] \right).$$

Tenemos ahora que $E_{D_{t-1}}[y_t] = a_t, E_{D_{t-1}}[\theta_t] = a_t$,
por definición,
y adicionalmente
$$Var_{D_{t-1}}(y_t) = Var_{D_{t-1}}(\theta_t) + \sigma_\epsilon^2 = P_t + \sigma_\epsilon^2$$
de modo que la ecuación de arriba queda:

$$E_{D_{t-1}}[\theta_t| y_t] = a_t + \rho\sqrt{\frac{P_t}{P_t + \sigma_\epsilon^2}}\left
(y_t - a_t \right).$$

Ahora consideramos $\rho$. La covarianza entre $\alpha_t$ y $y_t$ dado
$D_{t-1}$ es igual a
$$\textrm{Cov}_{D_{t-1}}(\theta_t, y_t) = E_{D_{t-1}}((\theta_t - a_t)(y_t -a_t))$$
que es igual a

$$E_{D_{t-1}}((\theta_t - a_t)(\theta_t + \epsilon_t - a_t)) =
\textrm{Var}_{D_{t-1}}(\theta_t) = P_t,$$

y sustituyendo arriba,

$$m_t = E[\theta_t|D_t]= E_{D_{t-1}}[\theta_t| y_t] = a_t + {\frac{P_t}{P_t + \sigma_\epsilon^2}}\left
(y_t - a_t \right) $$

es decir,

$$m_t =  a_t + K_t(y_t-a_t)$$
donde 

$$K_t = \frac{P_t}{P_t + \sigma_\epsilon^2}$$.

Y ahora podemos interpetar esta ecuación: para actualizar el valor
esperado del estado $a_t$ dado $D_{t-1}$,
una vez que observamos $y_t$, ajustamos $a_t$
por un factor de la *innovación*
$$v_t = y_t - a_t$$
El factor de ajuste es chico cuando la observación es muy ruidosa ($\sigma_\epsilon$ es grande), o cuando $P_t$ es muy chico (el estado es estimado con alta precisión).
Esto explica cómo el nivel es un tipo de suavizamiento de las observaciones.

Ahora calculamos la varianza. Por las ecuaciones de arriba de la normal bivariada,
tenemos entonces:

$$\rho^2 = \frac{P_t^2}{P_t(P_t + \sigma_\epsilon^2) } = \frac{P_t}{P_t + \sigma_\epsilon^2} $$
De modo que

$$C_t = \textrm{Var}(\theta_t|D_t) = P_t(1 - K_t)$$
Nótese que $K_t$ está entre cero y uno, y como 
$P_t =\textrm{Var}(\theta_t|D_{t-1})$,
la varianza del estado baja cuando observamos un nuevo valor $y_t$. A la cantidad
$K_t$ se le llama a veces ganancia de Kalman.

### Predicción

Finalmente, hacemos predicciones usando $p(\theta_{t}|D_{t-1})$. Como
esta última distribución es normal con media $a_t$ y varianza $P_t$, 
la distribución predictiva $p(y_{t}|D_{t-1})$ es normal con media

$$f_t = a_t$$

y varianza

$$P_t + \sigma_\epsilon^2$$

### Resumen

A este proceso iterativo de evolución y actualización se le llama
*filtro de Kalman*. Nuestro resultado es el siguiente:

::: callout-note
# Filtro de Kalman para nivel local

1. Suponemos $\theta_{t-1}|D_{t-1} \sim N(m_{t-1}, C_{t-1})$, con algunas $m_t$ y $C_t$ fijas.
2. **Predicción de estado**: $\theta_t|D_{t-1} \sim N(a_t, P_t)$, donde
$$a_t = m_{t-1}$$ y $P_t = C_{t-1} + \sigma_\eta^2$.
3. **Pronóstico a un paso**: $y_{t} \sim N(f_t, Q_t)$, donde
$f_t = a_t$ y $Q_t = P_t + \sigma_\epsilon^2$.
4. **Actualización**: Sea $\theta_t|D_{t} \sim N(m_t, C_t)$, donde
$m_t = a_t + K_t(y_t - a_t)$ y $C_t = P_t(1-K_t)$, con 
$K_t = \frac{P_t}{P_t + \sigma_{\epsilon}^2}$

Inicializamos el filtro fijando $\theta_1|D_0 \sim N(a_1, P_1).$ 
:::

Para hacer predicciones, este filtro es todo lo que necesitamos si 
$\sigma_{\epsilon},\sigma_{\nu}$ son conocidas. Este filtro puede implementarse
de manera muy eficiente. Puedes ver la versión general para el modelo
estructural lineal en 4.2 de @durbinkoopman.

### Ejemplo

Consideramos la serie de Río Nilo. Fijamos $\sigma_\epsilon = 120$
y $\sigma_\eta = 22$ (estimadores puntuales que calculamos arriba). El filtro
en una implementación simple se ve como sigue:

```{r}
filtro_nivel <- function(y, sigma_2_eps = 122^2, sigma_2_eta = 40^2){
  n <- length(y)
  a <- numeric(n)
  m <- numeric(n)
  P <- numeric(n)
  C <- numeric(n)
  Q <- numeric(n)
  v_std <- numeric(n)
  a[1] <- 1000
  P[1] <- 50
  for(t in 1:n){
    # predicción de estado
    if(t >= 2){
      a[t] = m[t-1]
      P[t] = C[t-1] + sigma_2_eta
    }
    # pronóstico
    y_f[t] <- a[t]
    Q[t] <- P[t] + sigma_2_eps
    # actualización
    K = P[t]/ (P[t] + sigma_2_eps)
    m[t] <- a[t] + K * (y[t] - a[t])
    C[t] <- P[t] * (1 - K)
    v_std[t] <- (y[t] - a[t])/sqrt(P[t] + sigma_2_eps)
  }
  tibble(t = 1:n, a, m, y_f, Q, P, C, y, v_std) |> 
    as_tsibble(index = t)
}
filtro_tbl <- filtro_nivel(Nile |> as.numeric())
```

En la siguiente gráfica, graficamos el pronóstico del estado para
cada $t$. Obsérvese como el filtro va corrigiendo (está haciendo "tracking" del
estado) su estimación dependiendo
de dónde caen las observaciones en relación al estado anterior:

```{r}
ggplot(filtro_tbl, 
  aes(x = t, ymin = m - 2 * sqrt(C), ymax = m + 2 * sqrt(C))) + 
  geom_ribbon(alpha = 0.5, fill = "red") + 
  geom_line(aes(y= a), colour = "red")+
  geom_line(aes(y = y)) + geom_point(aes(y = y))
```





## Innovaciones y diagnósticos

Un resultado importante de este filtro es que las innovaciones
$v_t = y_t - f_t$, que es comparar la observación con la predicción de $y_t$ dado $D_{t-1}$
cumplen la siguiente propiedad:


::: callout-tip
# Innovaciones y diagnósticos

Bajo los supuestos del modelo lineal de espacio de estados, las innovaciones
$v_t$ son normales con media cero y sin correlación.

:::

La normalidad puedes demostrarla del argumento de arriba. La media es cero pues
por la esperanza iterada,

$$E[y_t-f_t] = E[E[y_t - f_t|D_{t-1}]]$$
pero $E[y_t|D_{t-1}] = E[\theta_t - \eta_t|D_{t-1}] = a_t = f_t$.

Igualmente puedes condicionar para demostrar  covarianza cero: 
supongamos que $s < t$, entonces

$$E[(y_t - f_t)(y_s -f_s)|D_{t-1}] = (y_s - f_s)E[(y_t - f_t)|D_{t-1}]$$
pero $E[y_t-f_t|D_{t-1}]=0$ y por esperanza iterada, la covarianza de cualquier par $v_t,v_s$, $s\neq t$ es
igual a 0.

Esto nos da un diagnóstico de ajuste basado en las predicciones a un paso.
Por ejemplo, nuestro diagnóstico
para el modelo del río Nilo que vimos arriba da:

```{r}
#| layout-ncol: 2
#| fig-cap: 
#|  - "Fig 1"
#|  - "Fig 2"
#| fig-width: 4
#| fig-height: 3
ACF(filtro_tbl, v_std) |> autoplot() + ylim(c(-1, 1))
ggplot(filtro_tbl, aes(sample = v_std)) + geom_qq() +
  geom_qq_line()
```




## Filtrado y Suavizamiento

Las estimaciones del estado 
en línea del análisis secuencial generalmente se llaman **estados filtrados**,
mientras que las estimaciones del análisis retrospectivo que vimos arriba
se llaman **estados suavizados**.

Bajo los supuestos del modelo lineal con errores gaussianos es posible
hacer un algoritmo eficiente para calcular tanto filtro como suavizador
(este es el enfoque del paquete *bsts*). Para hacer una simulación MCMC
hacemos:

- Con algunos valores iniciales para las varianzas, se filtran los datos
con el filtro de Kalman. Se obtienen predicciones a un paso condicionales
a los valores de la varianza.
- Se obtienen los valores suavizados con otro algoritmo suavizador, que hace una pasada de los datos del final hacia el principio.
- Se obtiene una simulación de las varianzas condicional a los valores suavizados.
- Se repite el proceso para la siguiente simulación.

La ventaja de este enfoque es que es una manera eficiente de obtener todos
los estados suavizados junto con las innovaciones y error de predicción condicional
a las varianzas. Podemos usar diagnósticos de ajuste sobre esas innovaciones
que resultan del filtro.

### Ejemplo (bsts) {-}

```{r}
#| message: false
library(bsts)
y <- Nile
esp_estado <- AddLocalLevel(list(), y) 
ajuste <- bsts(y, state.specification = esp_estado,
               niter = 10000, seed = 2323)
```

Podemos ver el estado suavizado:

```{r}
plot(ajuste, "state",  burn = 2000)
```
Y podemos también examinar las innovaciones, que se obtienen aplicando
el filtro de Kalman para cada simulación de las varianzas y los coeficientes
de regresión:

```{r}
#| warning: false
pred_errors_tbl <- 
  ajuste$one.step.prediction.errors |> 
  t() |> as_tibble() |>
  mutate(t = 1: length(Nile)) |> 
  pivot_longer(-c(t), names_to = "sim", values_to = "valor") |> 
  group_by(t) |> 
  summarise(valor = mean(valor)) |> 
  as_tsibble(index = t)
```

```{r}
#| fig-width: 7
#| fig-height: 3
ACF(pred_errors_tbl) |> 
  autoplot() + ylim(c(-1,1))
```

